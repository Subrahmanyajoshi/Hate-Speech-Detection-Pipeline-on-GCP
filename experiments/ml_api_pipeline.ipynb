{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e20a701-6757-4320-9be2-0d08e01b5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import dill\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.auth\n",
    "\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions, StandardOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners import DirectRunner\n",
    "\n",
    "\n",
    "from apache_beam import DoFn, GroupByKey, io, ParDo, Pipeline, PTransform, WindowInto, WithKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42221bd6-744a-405a-8080-fb242a913e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions(streaming=True, save_main_session=True)\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow runs.\n",
    "options.view_as(GoogleCloudOptions).region = 'us-east1'\n",
    "\n",
    "options.view_as(GoogleCloudOptions).job_name = f'sa-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "dataflow_gcs_location = f'gs://text-analysis-323506/{options.view_as(GoogleCloudOptions).job_name}'\n",
    "\n",
    "# The directory to store the output files of the job.\n",
    "output_gcs_location = f\"{dataflow_gcs_location}/output\"\n",
    "\n",
    "# Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = f\"{dataflow_gcs_location}/staging\"\n",
    "\n",
    "# Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = f\"{dataflow_gcs_location}/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ed4df2-d248-49a2-bf9a-242845d36c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comment(message: bytes):\n",
    "    import json\n",
    "    message = json.loads(message.decode(\"utf-8\"))\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada3c4ce-8f0a-4027-a0e6-69a20b96ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comment(message: Dict):\n",
    "    line = message['text']\n",
    "\n",
    "    # Remove extra spaces, hastags and new line characters\n",
    "    line = line.strip()\n",
    "    line = line.replace('\\n', '')\n",
    "    line = line.replace('\\\\', '')\n",
    "    line = line.replace('#', '')\n",
    "    line = line.replace('&', ' ')\n",
    "    line = ' '.join(line.split())\n",
    "\n",
    "    # Href strings in comments\n",
    "    re.sub(\"<[^>]+>\", \"\", line)\n",
    "\n",
    "    # Remove @ mentions and URLs\n",
    "    line = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", line)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    line = \" \".join(line.split())\n",
    "\n",
    "    # Remove special characters\n",
    "    line = re.sub('[-+.^:,!]', '', line)\n",
    "\n",
    "    # Remove Numbers\n",
    "    line = ' '.join(c for c in line.split() if not c.isdigit())\n",
    "\n",
    "    # Remove Emojies\n",
    "    emoj = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u\"\\U00010000-\\U0010ffff\"\n",
    "    u\"\\u2640-\\u2642\" \n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\ufe0f\"  # dingbats\n",
    "    u\"\\u3030\"\n",
    "                  \"]+\", re.UNICODE)\n",
    "    line = re.sub(emoj, '', line)\n",
    "\n",
    "    line = line.lower().strip()\n",
    "    \n",
    "    # Expanding short forms\n",
    "    contraction_dict = {\"ain't\": \"are not\", \"'s\": \" is\", \"i'm\": \"i am\", \"aren't\": \"are not\", \"don't\": \"do not\",\n",
    "                        \"didn't\": \"did not\", \"won't\": \"will not\",\n",
    "                        \"can't\": \"cannot\", \"wouldn't\": \"would not\", \"hv\": \"have\", \"ik\": \"i know\", \"fr\": \"for real\"}\n",
    "\n",
    "    words = line.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in contraction_dict:\n",
    "            words[i] = contraction_dict[words[i]]\n",
    "    line = ' '.join(words)\n",
    "    \n",
    "    message['preprocessed'] = nlp.Document(line, type='PLAIN_TEXT')\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78cd0fa-39c1-42c2-a677-6a0b09ecb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sentiments(message: Dict):\n",
    "    from google.cloud import language\n",
    "    \n",
    "    client = language.LanguageServiceClient()\n",
    "    line = message['preprocessed']\n",
    "    \n",
    "    try:\n",
    "        message['response'] = client.analyze_sentiment(document={'content': line.content, 'type': line.type})\n",
    "    except Exception:\n",
    "        message['response'] = None\n",
    "        \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8b886a-769f-4eba-826c-9a3cd9bff98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(message):\n",
    "    import json\n",
    "    \n",
    "    response = message['response']\n",
    "    \n",
    "    if response:\n",
    "        message['score'] = response.document_sentiment.score\n",
    "        message['sentiment'] = 'hate' if message['score'] <= -0.6 else 'normal'\n",
    "    else:\n",
    "        message['score'] = np.nan\n",
    "        message['sentiment'] = 'NA'\n",
    "        \n",
    "    del message['preprocessed']\n",
    "    del message['response']\n",
    "    \n",
    "    print(message)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c589b7bf-29e8-4a43-a62f-9a347b2b539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Results into Hate comments or Normal comments\n",
    "\n",
    "class ResultsFilter(beam.DoFn):\n",
    "    \n",
    "    OUTPUT_TAG_HATE = 'Hate comments'\n",
    "    OUTPUT_TAG_NORM = 'Normal comments'\n",
    "        \n",
    "    def process(self, result):\n",
    "        import json\n",
    "        from apache_beam import pvalue\n",
    "        \n",
    "        sentiment = result['sentiment']\n",
    "        \n",
    "        if sentiment == 'hate':\n",
    "            yield pvalue.TaggedOutput(self.OUTPUT_TAG_HATE, result)\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput(self.OUTPUT_TAG_NORM, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12dbc66e-85be-41f3-b11b-f3d86553ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bytes(result):\n",
    "    import json\n",
    "    return json.dumps(result).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01540c45-c718-4cbe-848b-8f1806dffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline object\n",
    "pipeline = beam.Pipeline(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2e9d5-335f-4870-a572-65157748887a",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd57691-b273-4a44-b772-55cfc683c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-1c4e748c-1299-4b46-8213-651b83d7688d.json']\n"
     ]
    }
   ],
   "source": [
    "# Get tweet sentiments\n",
    "results =  (\n",
    "        pipeline\n",
    "        | 'From PubSub' >> io.gcp.pubsub.ReadFromPubSub(topic='projects/text-analysis-323506/topics/yt-comments')\n",
    "        | 'Load Comments' >> beam.Map(load_comment)\n",
    "        | 'Preprocess Comments' >> beam.Map(preprocess_comment)\n",
    "        | 'Detect Sentiments' >> beam.Map(detect_sentiments)\n",
    "        | 'Prepare Results' >> beam.Map(prepare_results)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c9623b5-c4a8-4180-80fd-4b18e57ad536",
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_results = (results | 'Divide Results' >> beam.ParDo(ResultsFilter()).with_outputs('Hate comments', 'Normal comments'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9e200-9ad4-42bf-93b8-b9b9051deef1",
   "metadata": {},
   "source": [
    "### Results to pubsub\n",
    "In this example we are sending only hate comments results to result pubsub topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ddbd25-30cc-497b-831b-74fd298c2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hate comments results to PubSub topic\n",
    "hate_comments_pubsub = (\n",
    "                    separated_results['Hate comments']\n",
    "                    | 'Bytes Conversion' >> beam.Map(convert_to_bytes)\n",
    "                    | 'PS Hate Comments' >> beam.io.WriteToPubSub(topic='projects/text-analysis-323506/topics/hs-results')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7c0c5-692e-4df8-9e3a-49c0133d48cc",
   "metadata": {},
   "source": [
    "### Results to Bigquery\n",
    "\n",
    "We will send normal comments and hate comments to separate tables. These can then be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b553b280-e5f5-4dd2-af32-f923fd77bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "        bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"text\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"user_name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"user_id\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"user_profile\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"video_id\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"score\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"sentiment\", \"STRING\", mode=\"NULLABLE\")\n",
    "    ]\n",
    "\n",
    "hate_comments_bq = (\n",
    "                separated_results['Hate comments']\n",
    "                | 'BQ Hate Comments' >> beam.io.WriteToBigQuery(table='hate_comments', dataset='yt_comments_analysis', project='text-analysis-323506')\n",
    "            )   \n",
    "\n",
    "normal_comments_bq = (\n",
    "                separated_results['Normal comments']\n",
    "                | 'BQ Norm Comments' >> beam.io.WriteToBigQuery(table='normal_comments', dataset='yt_comments_analysis', project='text-analysis-323506')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1dd86c-9e0a-40d3-9709-89c1a281c498",
   "metadata": {},
   "source": [
    "### Direct Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb107c2d-ee02-42f9-b584-e5b11c9b9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DirectRunner().run_pipeline(pipeline, options=options).wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e496413-ef9e-45f9-b26d-60ffb92d6fab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DataFlow Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99951473-8891-43a8-b9ab-af966527515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-1c4e748c-1299-4b46-8213-651b83d7688d.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-1c4e748c-1299-4b46-8213-651b83d7688d.json']\n"
     ]
    }
   ],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(pipeline, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69377d2c-de34-4819-b579-3fcf31b212cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.37.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.37.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
