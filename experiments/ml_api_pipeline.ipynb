{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e20a701-6757-4320-9be2-0d08e01b5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.auth\n",
    "\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions, StandardOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import language\n",
    "from apache_beam.ml.gcp import naturallanguageml as nlp\n",
    "\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners import DirectRunner\n",
    "\n",
    "\n",
    "from apache_beam import DoFn, GroupByKey, io, ParDo, Pipeline, PTransform, WindowInto, WithKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42221bd6-744a-405a-8080-fb242a913e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions(streaming=True, save_main_session=True)\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow runs.\n",
    "options.view_as(GoogleCloudOptions).region = 'us-east1'\n",
    "\n",
    "options.view_as(GoogleCloudOptions).job_name = f'sa-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "dataflow_gcs_location = f'gs://text-analysis-323506/{options.view_as(GoogleCloudOptions).job_name}'\n",
    "\n",
    "# The directory to store the output files of the job.\n",
    "output_gcs_location = f\"{dataflow_gcs_location}/output\"\n",
    "\n",
    "# Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = f\"{dataflow_gcs_location}/staging\"\n",
    "\n",
    "# Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = f\"{dataflow_gcs_location}/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ed4df2-d248-49a2-bf9a-242845d36c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweet(message: bytes):\n",
    "    import json\n",
    "    message = json.loads(message.decode(\"utf-8\"))\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada3c4ce-8f0a-4027-a0e6-69a20b96ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(message: Dict):\n",
    "    from apache_beam.ml.gcp import naturallanguageml as nlp\n",
    "    import re\n",
    "    import string\n",
    "\n",
    "    line = message['content']\n",
    "    \n",
    "    # Remove extra spaces, hastags and new line characters\n",
    "    line = line.strip()\n",
    "    line = line.replace('\\n','')\n",
    "    line = line.replace('\\\\','')\n",
    "    line = line.replace('#','')\n",
    "    line = ' '.join(line.split())\n",
    "    \n",
    "    # Remove @ mentions and URLs\n",
    "    line = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", line)\n",
    "    line = \" \".join(line.split())\n",
    "    \n",
    "    # Expanding short forms\n",
    "    contraction_dict = {\"ain't\": \"are not\", \"'s\":\" is\", \"aren't\": \"are not\", \"don't\": \"do not\", \"didn't\": \"did not\", \"won't\": \"will not\", \n",
    "                   \"can't\": \"cannot\"}\n",
    "    \n",
    "    words = line.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in contraction_dict:\n",
    "            words[i] = contraction_dict[words[i]]\n",
    "    line = ' '.join(words)\n",
    "    \n",
    "    # Remove special characters\n",
    "    line = re.sub('[-+.^:,]','',line)\n",
    "    \n",
    "    message['preprocessed'] = line\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78cd0fa-39c1-42c2-a677-6a0b09ecb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = language.LanguageServiceClient()\n",
    "\n",
    "def detect_sentiments(message: Dict):\n",
    "    global client\n",
    "    line = message['preprocessed']\n",
    "    line = nlp.Document(line, type='PLAIN_TEXT')\n",
    "    \n",
    "    try:\n",
    "        message['response'] = client.analyze_sentiment(document={'content': line.content, 'type': line.type})\n",
    "    except Exception:\n",
    "        message['response'] = None\n",
    "        \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8b886a-769f-4eba-826c-9a3cd9bff98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(message):\n",
    "    import json\n",
    "    \n",
    "    response = message['response']\n",
    "    \n",
    "    if response:\n",
    "        message['score'] = response.document_sentiment.score\n",
    "        message['sentiment'] = 'hate' if message['score'] < -0.25 else 'normal'\n",
    "    else:\n",
    "        message['score'] = np.nan\n",
    "        message['sentiment'] = 'NA'\n",
    "        \n",
    "    del message['preprocessed']\n",
    "    del message['response']\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01540c45-c718-4cbe-848b-8f1806dffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline object\n",
    "pipeline = beam.Pipeline(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2e9d5-335f-4870-a572-65157748887a",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd57691-b273-4a44-b772-55cfc683c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweet sentiments\n",
    "results =  (\n",
    "        pipeline\n",
    "        | 'Consume Messages' >> io.gcp.pubsub.ReadFromPubSub(topic='projects/text-analysis-323506/topics/tweets')\n",
    "        | 'Load Tweets' >> beam.Map(load_tweet)\n",
    "        | 'Preprocess Tweets' >> beam.Map(preprocess_tweet)\n",
    "        | 'Detect Sentiments' >> beam.Map(detect_sentiments)\n",
    "        | 'Prepare Results' >> beam.Map(prepare_results)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb5d37-561e-47c3-ab33-6c66a1c14c8f",
   "metadata": {},
   "source": [
    "### Separate Results into Hate speech or Normal speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c84b530-377c-4cf0-8693-607420ae8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsFilter(beam.DoFn):\n",
    "    \n",
    "    OUTPUT_TAG_HATE = 'Hate speech'\n",
    "    OUTPUT_TAG_NORM = 'Normal speech'\n",
    "    \n",
    "    def process(self, result):\n",
    "        import json\n",
    "        sentiment = result['sentiment']\n",
    "        \n",
    "        if sentiment == 'hate':\n",
    "            yield pvalue.TaggedOutput(ResultsFilter.OUTPUT_TAG_HATE, result)\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput(ResultsFilter.OUTPUT_TAG_NORM, result)\n",
    "            \n",
    "separated_results = (results | beam.ParDo(ResultsFilter()).with_outputs(ResultsFilter.OUTPUT_TAG_HATE, ResultsFilter.OUTPUT_TAG_NORM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9e200-9ad4-42bf-93b8-b9b9051deef1",
   "metadata": {},
   "source": [
    "### Results to pubsub\n",
    "In this example we are sending only hate speech results to result pubsub topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7ddbd25-30cc-497b-831b-74fd298c2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bytes(result):\n",
    "    import json\n",
    "    return json.dumps(result).encode(\"utf-8\")\n",
    "\n",
    "# Hate speech results to PubSub topic\n",
    "hate_speech_pubsub = (\n",
    "                    separated_results[ResultsFilter.OUTPUT_TAG_HATE]\n",
    "                    | 'Bytes Conversion' >> beam.Map(convert_to_bytes)\n",
    "                    | 'To PubSub Topic' >> beam.io.WriteToPubSub(topic='projects/text-analysis-323506/topics/sa-results')\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7c0c5-692e-4df8-9e3a-49c0133d48cc",
   "metadata": {},
   "source": [
    "### Results to Bigquery\n",
    "\n",
    "We will send normal speech and hate speech to separate tables. These can then be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b553b280-e5f5-4dd2-af32-f923fd77bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/apache-beam-2.35.0/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2106: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  is_streaming_pipeline = p.options.view_as(StandardOptions).streaming\n"
     ]
    }
   ],
   "source": [
    "schema = [\n",
    "        bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"id\", \"BIGNUMERIC\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"username\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"score\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"sentiment\", \"STRING\", mode=\"REQUIRED\")\n",
    "    ]\n",
    "\n",
    "hate_speech_bq = (\n",
    "                separated_results[ResultsFilter.OUTPUT_TAG_HATE]\n",
    "                | 'To hate speech table' >> beam.io.WriteToBigQuery(table='hate_speeches', dataset='tweets_analysis', project='text-analysis-323506')\n",
    "            )\n",
    "\n",
    "normal_speech_bq = (\n",
    "                separated_results[ResultsFilter.OUTPUT_TAG_NORM]\n",
    "                | 'To norm speech table' >> beam.io.WriteToBigQuery(table='normal_speeches', dataset='tweets_analysis', project='text-analysis-323506')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1dd86c-9e0a-40d3-9709-89c1a281c498",
   "metadata": {},
   "source": [
    "### Direct Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb107c2d-ee02-42f9-b584-e5b11c9b9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DirectRunner().run_pipeline(pipeline, options=options).wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e496413-ef9e-45f9-b26d-60ffb92d6fab",
   "metadata": {},
   "source": [
    "### DataFlow Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99951473-8891-43a8-b9ab-af966527515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(pipeline, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69377d2c-de34-4819-b579-3fcf31b212cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03. Apache Beam 2.35.0 for Python 3",
   "language": "python",
   "name": "03-apache-beam-2.35.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
